import math
import sys

# ==========================================
# 1. åŸºç¡€æ•°æ®åº“
# ==========================================

GPU_SPECS = {
    "1": {"name": "NVIDIA H100 (80GB)", "vram": 80, "bw": 3350, "tflops": 989, "desc": "HBM3, æ–°ä¸€ä»£æ——èˆ°, FP8ç®—åŠ›å¼ºæ‚"},
    "2": {"name": "NVIDIA A100 (80GB)", "vram": 80, "bw": 1935, "tflops": 312, "desc": "HBM2e, å·¥ä¸šç•Œæ ‡å‡†è®­ç»ƒ/æ¨ç†å¡"},
    "3": {"name": "NVIDIA A100 (40GB)", "vram": 40, "bw": 1555, "tflops": 312, "desc": "HBM2, æ˜¾å­˜è¾ƒå°"},
    "4": {"name": "NVIDIA H20 (96GB)",  "vram": 96, "bw": 4000, "tflops": 148, "desc": "HBM3, ä¸­å›½ç‰¹ä¾›, é«˜å¸¦å®½ä½ç®—åŠ›"},
    "5": {"name": "RTX 4090 (24GB)",    "vram": 24, "bw": 1008, "tflops": 330, "desc": "GDDR6X, æ¶ˆè´¹çº§æœ€å¼º"},
    "6": {"name": "Apple M3 Max (128G)", "vram": 128, "bw": 400, "tflops": 15, "desc": "ç»Ÿä¸€å†…å­˜, é€‚åˆæœ¬åœ°æ¨ç†"},
    "7": {"name": "åä¸º Ascend 910C",    "vram": 128, "bw": 3200, "tflops": 800, "desc": "HBM3, å›½äº§ç®—åŠ›æ——èˆ°"},
    "8": {"name": "æµ·å…‰ DCU K100",      "vram": 64,  "bw": 892,  "tflops": 196, "desc": "HBM3, å…¼å®¹ ROCm ç”Ÿæ€"},
    "9": {"name": "å¯’æ­¦çºª MLU590",      "vram": 80,  "bw": 2000, "tflops": 314, "desc": "HBM2e, ç±»ä¼¼ A100 æ€§èƒ½"},
    "10": {"name": "L40S (48GB)",       "vram": 48,  "bw": 864,  "tflops": 366, "desc": "GDDR6, æ¨ç†ä¸“ç”¨, æ— NVLink"}
}

# æƒé‡ç²¾åº¦ (Weight Precision)
WEIGHT_PRECISION = {
    "1": {"name": "FP16 (åŠç²¾åº¦)", "bytes": 2},
    "2": {"name": "INT8 (8-bit)",   "bytes": 1},
    "3": {"name": "INT4 (4-bit)",   "bytes": 0.5},
    "4": {"name": "AWQ/GPTQ",       "bytes": 0.55}
}

# KV Cache ç²¾åº¦ (KV Cache Precision)
KV_PRECISION = {
    "1": {"name": "FP16 (æ ‡å‡†)",       "bytes": 2},
    "2": {"name": "INT8 (KVé‡åŒ–)",     "bytes": 1}, 
    "3": {"name": "FP8 (H100ä¼˜åŒ–)",    "bytes": 1} 
}

# KV Cache åŸºå‡†å€¼ (MB per token, åŒ…å« GQA ä¼˜åŒ–)
# æ•°æ®æ¥æºï¼švLLM/TRT-LLM å®æµ‹æ•°æ®
MODEL_ARCH_BASE = {
    "7B":   0.18,  # Llama-3-8B / Qwen2-7B
    "14B":  0.28,  # Qwen2-14B
    "32B":  0.38,  # Yi-34B
    "72B":  0.50,  # Qwen2-72B / Llama-3-70B
    "110B": 0.65   # Qwen2-110B
}

# ç³»ç»Ÿæ•ˆç‡å‚æ•° (åŸºäº vLLM/FlashAttention-2)
MBU_EFFICIENCY = 0.70           # æ˜¾å­˜å¸¦å®½æœ‰æ•ˆåˆ©ç”¨ç‡
COMPUTE_EFFICIENCY = 0.32       # ç®—åŠ›æœ‰æ•ˆåˆ©ç”¨ç‡ (Prefillé˜¶æ®µ)
EFFECTIVE_LOAD_FACTOR = 0.80    # æ˜¾å­˜è§„åˆ’è´Ÿè½½å› å­ (é¢„ç•™æ³¢åŠ¨ç©ºé—´)

# ==========================================
# 2. æ ¸å¿ƒè®¡ç®—é€»è¾‘ (V4 ç‰©ç†ä¿®æ­£ç‰ˆ)
# ==========================================

def calculate_production_grade_v4(inputs):
    """
    ç”Ÿäº§çº§ GPU è§„åˆ’è®¡ç®—æ ¸å¿ƒ - V4
    åŸºäº Roofline æ¨¡å‹ä¸æ­£ç¡®çš„ Batching ç‰©ç†æœºåˆ¶
    """
    
    # --- A. æ˜¾å­˜å®¹é‡è®¡ç®— ---
    
    param_b = inputs['param_size']
    # å·¥ç¨‹è¿‘ä¼¼: 1B params * 2 bytes â‰ˆ 2GB (è¯¯å·®è¢« Buffer å¸æ”¶)
    weight_gb = param_b * inputs['weight_bytes']
    
    # KV Cache è®¡ç®—
    base_kv_mb = MODEL_ARCH_BASE.get(inputs['model_scale'], 0.50)
    kv_scale = inputs['kv_bytes'] / 2.0
    kv_mb_per_token = base_kv_mb * kv_scale
    
    concurrency = inputs['concurrency']
    avg_seq = inputs['avg_context']
    
    # å®¹é‡è§„åˆ’ä½¿ç”¨å³°å€¼è´Ÿè½½
    total_tokens_capacity = avg_seq * concurrency * EFFECTIVE_LOAD_FACTOR
    kv_cache_gb = total_tokens_capacity * kv_mb_per_token / 1024
    
    # Buffer (15% Overhead: PyTorch context, fragmentation, activations)
    buffer_gb = 3.0 + (weight_gb + kv_cache_gb) * 0.15
    
    total_vram = weight_gb + kv_cache_gb + buffer_gb
    
    # --- B. ç¡¬ä»¶éœ€æ±‚è®¡ç®— ---
    
    gpu_vram = inputs['gpu']['vram']
    num_gpus = math.ceil(total_vram / gpu_vram)
    vram_util = total_vram / (num_gpus * gpu_vram)
    
    # --- C. æ€§èƒ½è®¡ç®— (ä¿®æ­£åçš„ç‰©ç†æ¨¡å‹) ---
    
    system_bw = inputs['gpu']['bw'] * num_gpus * MBU_EFFICIENCY
    
    # 1. åŸºç¡€æ­¥é€Ÿ (Base Steps/s)
    # ç‰©ç†æ„ä¹‰ï¼šä¸è€ƒè™‘ KV å’Œé€šä¿¡ï¼Œä»…è¯»å–æƒé‡èƒ½è·‘å¤šå¿«ï¼Ÿ
    # è¿™æ˜¯å•ç”¨æˆ·é€Ÿåº¦çš„ç†è®ºä¸Šé™ã€‚
    if weight_gb > 0:
        base_steps_per_sec = system_bw / weight_gb
    else:
        base_steps_per_sec = 0
        
    # 2. æ•ˆç‡ä¿®æ­£å› å­
    
    # (a) Batch æ•ˆç‡: å¹¶å‘è¿‡ä½æ— æ³•å–‚é¥± GPU
    if concurrency < 4:
        batch_eff = 0.5 + 0.125 * concurrency
    else:
        batch_eff = 1.0
        
    # (b) TP é€šä¿¡æŸè€—: å¤šå¡äº’è”çš„ overhead
    if num_gpus > 1:
        # NVLink ä¹Ÿä¼šæœ‰æŸè€—ï¼Œå‡è®¾æ¯å¡å¢åŠ  5% æŸè€—ï¼Œæœ€å¤š 30%
        tp_eff = max(0.70, 1.0 - 0.05 * (num_gpus - 1))
    else:
        tp_eff = 1.0
        
    # (c) KV Cache æµé‡æƒ©ç½š (Traffic Penalty)
    # åœ¨ Decode é˜¶æ®µï¼Œæ¯ä¸€æ­¥ä¼ è¾“çš„æ•°æ® = æƒé‡ + æ´»è·ƒKV
    # æ´»è·ƒ KV = å¹¶å‘æ•° * å¹³å‡å†å²é•¿åº¦ * KVå¤§å°
    avg_history_len = avg_seq / 2
    active_kv_gb = concurrency * avg_history_len * (kv_mb_per_token / 1024)
    
    # æµé‡è†¨èƒ€ç³»æ•°
    traffic_ratio = (weight_gb + active_kv_gb) / weight_gb
    
    # 3. æœ€ç»ˆé€Ÿåº¦è®¡ç®—
    
    # å•ç”¨æˆ·é€Ÿåº¦ (User Speed): çœŸå®çš„ç”Ÿæˆä½“éªŒ
    user_speed = (base_steps_per_sec * batch_eff * tp_eff) / traffic_ratio
    
    # ç³»ç»Ÿæ€»åå (System Throughput): æœåŠ¡æ‰¿è½½èƒ½åŠ›
    system_throughput = user_speed * concurrency
    
    # --- D. å»¶è¿Ÿä¼°ç®— (Prefill & E2E) ---
    
    # Prefill (Compute Bound)
    avg_input_tokens = avg_seq * 0.75
    prefill_flops = 2 * param_b * 1e9 * avg_input_tokens
    system_tflops = inputs['gpu']['tflops'] * num_gpus * COMPUTE_EFFICIENCY
    if system_tflops > 0:
        prefill_latency = prefill_flops / (system_tflops * 1e12)
    else:
        prefill_latency = 999
        
    # Decode Time
    expected_output_tokens = avg_seq * 0.25
    if user_speed > 0:
        decode_time = expected_output_tokens / user_speed
    else:
        decode_time = 999
        
    e2e_latency = prefill_latency + decode_time
    
    # --- E. è¯„çº§ ---
    if user_speed >= 40: grade, comment = "ğŸš€ æé€Ÿ", "è¶…æµç•… (äººç±»é˜…è¯»é€Ÿåº¦ 3-4å€)"
    elif user_speed >= 20: grade, comment = "ğŸŸ¢ ä¼˜ç§€", "æµç•…äº¤äº’ä½“éªŒ"
    elif user_speed >= 10: grade, comment = "ğŸŸ¡ è‰¯å¥½", "å¯æ¥å—çš„é˜…è¯»é€Ÿåº¦"
    else: grade, comment = "ğŸ”´ è¾ƒå·®", "æ˜æ˜¾çš„é€å­—ç”Ÿæˆæ„Ÿ"

    return {
        "capacity": {
            "total_vram": total_vram,
            "weight": weight_gb,
            "kv_cache": kv_cache_gb,
            "buffer": buffer_gb,
            "kv_mb_per_token": kv_mb_per_token
        },
        "hardware": {
            "num_gpus": num_gpus,
            "vram_util": vram_util,
            "total_vram_pool": num_gpus * gpu_vram
        },
        "performance": {
            "base_steps": base_steps_per_sec,
            "user_speed": user_speed,
            "system_throughput": system_throughput,
            "kv_traffic_ratio": traffic_ratio,
            "tp_efficiency": tp_eff,
            "prefill_latency": prefill_latency,
            "e2e_latency": e2e_latency,
            "grade": grade,
            "comment": comment
        },
        "bottleneck_analysis": {
            "memory_bound": active_kv_gb > weight_gb * 0.5,
            "compute_bound": prefill_latency > 2.0,
            "tp_constrained": tp_eff < 0.85
        }
    }

# ==========================================
# 3. äº¤äº’å·¥å…·å‡½æ•°
# ==========================================

def get_choice(options, text, default_key="1"):
    """æ”¯æŒå›è½¦é»˜è®¤å’Œé”™è¯¯é‡è¯•çš„é€‰æ‹©å‡½æ•°"""
    print(f"\n{text}")
    for k, v in options.items():
        name = v['name'] if 'name' in v else v
        suffix = " (é»˜è®¤)" if k == default_key else ""
        print(f"  [{k}] {name}{suffix}")
    
    while True:
        val = input(f"ğŸ‘‰ é€‰æ‹© [é»˜è®¤ {default_key}]: ").strip()
        if not val:
            val = default_key
        
        if val in options:
            return options[val]
        else:
            print(f"âŒ è¾“å…¥æ— æ•ˆï¼Œè¯·ä» {list(options.keys())} ä¸­é€‰æ‹©")

def get_number(prompt, default):
    val = input(f"{prompt} [é»˜è®¤ {default}]: ").strip()
    return float(val) if val else default

def get_closest_scale(param):
    scales = [7, 14, 32, 72, 110]
    closest = min(scales, key=lambda x: abs(x - param))
    return f"{closest}B"

# ==========================================
# 4. ä¸»ç¨‹åº
# ==========================================

def main():
    print("\n" + "="*60)
    print("ğŸš€ GPU ç”Ÿäº§çº§ç®—åŠ›è§„åˆ’å™¨ (Final V4)")
    print("   Physical-Based Modeling | Corrected Batching Logic")
    print("="*60)
    
    # --- è¾“å…¥ ---
    print("\n--- [1] æ¨¡å‹é…ç½® ---")
    param = get_number("æ¨¡å‹å‚æ•°é‡ (Billion)", 72)
    model_scale = get_closest_scale(param)
    print(f"   â†’ åŒ¹é…æ¶æ„åŸºå‡†: {model_scale}")
    
    w_prec = get_choice(WEIGHT_PRECISION, "æƒé‡ç²¾åº¦:")
    kv_prec = get_choice(KV_PRECISION, "KV Cache ç²¾åº¦:")
    
    print("\n--- [2] ä¸šåŠ¡è´Ÿè½½ ---")
    conc = int(get_number("å¹¶å‘ç”¨æˆ·æ•° (Concurrency)", 20))
    seq = int(get_number("å¹³å‡ä¸Šä¸‹æ–‡é•¿åº¦ (Input+Output)", 4096))
    
    print("\n--- [3] ç¡¬ä»¶é€‰æ‹© ---")
    gpu = get_choice(GPU_SPECS, "ç›®æ ‡æ˜¾å¡:")
    
    # --- è®¡ç®— ---
    print("\nâ³ æ­£åœ¨è¿›è¡Œç‰©ç†ä»¿çœŸè®¡ç®—...")
    res = calculate_production_grade_v4({
        'param_size': param,
        'model_scale': model_scale,
        'weight_bytes': w_prec['bytes'],
        'kv_bytes': kv_prec['bytes'],
        'avg_context': seq,
        'concurrency': conc,
        'gpu': gpu
    })
    
    c = res['capacity']
    h = res['hardware']
    p = res['performance']
    b = res['bottleneck_analysis']
    
    # --- æŠ¥å‘Š ---
    print("\n" + "="*60)
    print("ğŸ“Š ä»¿çœŸè¯„ä¼°æŠ¥å‘Š")
    print("="*60)
    
    # 1. å®¹é‡
    print(f"\n[1] æ˜¾å­˜å®¹é‡è§„åˆ’:")
    print(f"  â€¢ æ¨¡å‹æƒé‡:     {c['weight']:>8.2f} GB ({w_prec['name']})")
    print(f"  â€¢ KV Cache:     {c['kv_cache']:>8.2f} GB ({kv_prec['name']})")
    print(f"    â””â”€ å³°å€¼ä¼°ç®—:  {conc}å¹¶å‘ Ã— {seq}é•¿åº¦ Ã— {c['kv_mb_per_token']:.2f}MB/token")
    print(f"  â€¢ ç³»ç»ŸBuffer:   {c['buffer']:>8.2f} GB (é¢„ç•™ 15%)")
    print(f"  {'-'*40}")
    print(f"  â˜… æ€»æ˜¾å­˜éœ€æ±‚:   {c['total_vram']:>8.2f} GB")
    
    # 2. ç¡¬ä»¶
    print(f"\n[2] ç¡¬ä»¶é…ç½®å»ºè®®:")
    print(f"  â€¢ æ¨èé…ç½®:     {h['num_gpus']} Ã— {gpu['name']}")
    print(f"  â€¢ æ˜¾å­˜æ± :       {h['total_vram_pool']:.0f} GB")
    print(f"  â€¢ åˆ©ç”¨ç‡:       {h['vram_util']*100:.1f}%")
    
    if h['vram_util'] > 0.90:
        print("  âš ï¸  è­¦å‘Š: æ˜¾å­˜æå…¶ç´§å¼ ï¼Œå»ºè®®å¢åŠ  1 å¼ å¡é˜²æ­¢ OOM")
    elif h['vram_util'] < 0.60:
        print("  ğŸ’¡ æç¤º: æ˜¾å­˜æœ‰å¤§é‡å¯Œä½™ï¼Œå¯å°è¯•æ›´å¤§æ¨¡å‹æˆ–æ›´é«˜å¹¶å‘")
        
    # 3. æ€§èƒ½
    print(f"\n[3] æ€§èƒ½è¡¨ç° (å…³é”®æŒ‡æ ‡):")
    
    print(f"\n  ğŸ‘¤ å•ç”¨æˆ·ä½“éªŒ (User Speed):")
    print(f"     {p['user_speed']:>6.1f} tokens/s  [{p['grade']}]")
    print(f"     â””â”€ {p['comment']}")
    print(f"     â€¢ Prefillå»¶è¿Ÿ: {p['prefill_latency']:.2f} s (é¦–å­—ç­‰å¾…)")
    print(f"     â€¢ ç«¯åˆ°ç«¯å»¶è¿Ÿ:  {p['e2e_latency']:.2f} s")
    
    print(f"\n  ğŸ“ˆ ç³»ç»Ÿæ€»åå (System Throughput):")
    print(f"     {p['system_throughput']:>6.1f} tokens/s")
    print(f"     â””â”€ æ¯å¤©å¯å¤„ç†çº¦ {int(p['system_throughput']*3600*24/1e6)}M tokens")
    
    print(f"\n  âš™ï¸  æ•ˆç‡å› å­åˆ†æ:")
    print(f"     â€¢ åŸºå‡†æ­¥é€Ÿ: {p['base_steps']:.1f} steps/s (å¸¦å®½/æƒé‡)")
    print(f"     â€¢ TPé€šä¿¡æŸè€—: {(1-p['tp_efficiency'])*100:.0f}%")
    print(f"     â€¢ KVæµé‡æƒ©ç½š: é€Ÿåº¦é™ä½ {(p['kv_traffic_ratio']-1)*100:.0f}% (å› æ¬è¿KV Cache)")

    # 4. ç“¶é¢ˆä¸å»ºè®®
    print(f"\n[4] ç“¶é¢ˆè¯Šæ–­ä¸å»ºè®®:")
    
    has_issue = False
    
    if b['memory_bound']:
        print(f"  ğŸ”´ å†…å­˜å¸¦å®½ç“¶é¢ˆ: KV Cache ä¼ è¾“é‡è¿‡å¤§")
        print(f"     â†’ æ–¹æ¡ˆ: å¯ç”¨ {kv_prec['name']} -> INT8/FP8 KV Cache")
        print(f"     â†’ æ–¹æ¡ˆ: ä½¿ç”¨ GQA/MLA æ¶æ„æ¨¡å‹ (å¦‚ DeepSeek/Llama3)")
        has_issue = True
        
    if b['compute_bound']:
        print(f"  ğŸ”´ ç®—åŠ›ç“¶é¢ˆ: Prefill é˜¶æ®µè¿‡æ…¢")
        print(f"     â†’ æ–¹æ¡ˆ: å¢åŠ  GPU æ•°é‡åˆ©ç”¨ TP èšåˆç®—åŠ›")
        has_issue = True
        
    if b['tp_constrained']:
        print(f"  ğŸŸ¡ é€šä¿¡ç“¶é¢ˆ: å¤šå¡é€šä¿¡æŸè€—æ˜¾è‘—")
        print(f"     â†’ æ–¹æ¡ˆ: å¿…é¡»ä½¿ç”¨ NVLink/NVSwitchï¼Œé¿å… PCIe")
        has_issue = True
        
    if not has_issue:
        print(f"  âœ… ç³»ç»Ÿé…ç½®å‡è¡¡ï¼Œæ— æ˜æ˜¾ç¡¬ä»¶ç“¶é¢ˆ")
        
    # 5. æ¶æ„æ¨è
    print(f"\n[5] éƒ¨ç½²æ¶æ„æ¨è:")
    if h['num_gpus'] == 1:
        print("  ğŸ—ï¸  å•å¡æ¨ç† (Single GPU)")
        print("     â€¢ æ¨èå¼•æ“: vLLM, TensorRT-LLM")
    elif h['num_gpus'] <= 8:
        print(f"  ğŸ—ï¸  å¼ é‡å¹¶è¡Œ (Tensor Parallel, TP={h['num_gpus']})")
        print("     â€¢ å¿…é¡»æ‹¥æœ‰é«˜å¸¦å®½äº’è” (NVLink)")
        print(f"     â€¢ å¯åŠ¨å‘½ä»¤å‚è€ƒ: vllm serve ... --tensor-parallel-size {h['num_gpus']}")
    else:
        print(f"  ğŸ—ï¸  æ··åˆå¹¶è¡Œ (TP=8 + PP={math.ceil(h['num_gpus']/8)})")
        print("     â€¢ é€‚ç”¨äºè¶…å¤§è§„æ¨¡é›†ç¾¤ï¼Œéœ€å¤æ‚ç¼–æ’")

    print("\n" + "="*60)
    print("âœ… è®¡ç®—å®Œæˆ")
    print("="*60)

if __name__ == "__main__":
    main()
